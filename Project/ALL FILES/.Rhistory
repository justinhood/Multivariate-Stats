problem2<-function(n,alpha,to){
+ zstar=qnorm(1-alpha/2,0,1)
+ x=0:n
+ p=dbinom(x,n,to)
+ thetahat=x/n
+ li=thetahat-zstar*sqrt(thetahat*(1-thetahat)/n)
+ ui=thetahat+zstar*sqrt(thetahat*(1-thetahat)/n)
+lii=(thetahat+(zstar^2/(2*n))-zstar*sqrt(thetahat*(1-thetahat)/n+(zstar/(2*n))^2))/(1+zstar^2/n)
+uii=(thetahat+(zstar^2/(2*n))+zstar*sqrt(thetahat*(1-thetahat)/n+(zstar/(2*n))^2))/(1+zstar^2/n)
+ttil=(x+2)/(n+4)
+ntil=n+4
+liii=ttil-zstar*sqrt(ttil*(1-ttil)/ntil)
+uiii=ttil+zstar*sqrt(ttil*(1-ttil)/ntil)
+li[li<0]=0
+lii[lii<0]=0
+liii[liii<0]=0
+ui[ui>1]=1
+uii[uii>1]=1
+uiii[uiii>1]=1
+widthi=ui-li
+widthii=uii-lii
+widthiii=uiii-liii
+coversi=(li-to)*(ui-to)<0
+coversii=(lii-to)*(uii-to)<0
+coversiii=(liii-to)*(uii-to)<0
+coveri=coversi*p
+coverii=coversii*p
+coveriii=coversiii*p
+probi=sum(coveri)
+probii=sum(coverii)
+probiii=sum(coveriii)
+widei=sum(widthi*p)
+wideii=sum(widthii*p)
+wideiii=sum(widthiii*p)
+print("Method i probability",quote=FALSE)
+print(probi)
+print("Method i expected width",quote=FALSE)
+print(widei)
+print("Method ii probability",quote=FALSE)
+print(probii)
+print("Method ii expected width",quote=FALSE)
+print(wideii)
+print("Method iii probability",quote=FALSE)
+print(probiii)
+print("Method iii expected width",quote=FALSE)
+print(wideiii)}
problem2(10,.05,0.5)
q()
D=c(4.5e-5,2.1e-5,1.92e-5,6.6e-6,2.1e-6,1.14e-6,5.1e-7,6.03e-7,6.9e-7,4.1e-7,3.46e-7,1.97e-7,1.05e-7,5.3e-8)
M=c(1,32,48,192,5734,13370,16900,66500,64500,247500,482700,330000,524800,40590000)
fit<-lm(log(D)~log(M))
summary fit
summary(fit)
q()
x=[2,2
x=[2,2
[2,1][2,2]
clerra
clear
x=matrix(c(2,2,-1,3,1,1,2,0),nrow=4,ncol=2,byrow=TRUE)
x
y=c(5,3,2,4)
y
m1=lm(y~1)
summary(m1)
m2=lm(y~x[,2])
summary(m2)
m2=lm(y~x[,2]-1)
summary(m2)
m2=lm(y~x[,1]-1)
summary(m2)
full=lm(y~x)
summary(full)
m21=lm(Y~x[:,1])
m21=lm(Y~x[,1])
m21=lm(y~x[,1])
summary(m21)
m22=lm(y~x[,2])
summary(m22)
q()
clear
clc
y=[1979,1980,1981,1982,1983,1984,1985,1986]
y=(1979,1980,1981,1982,1983,1984,1985,1986)
y=(1979 1980 1981 1982 1983 1984 1985 1986)
clear
y->(1979,1980,1981,1982,1983,1984,1985,1986)
y->[1979,1980,1981,1982,1983,1984,1985,1986]
y=c(1979 1980 1981 1982 1983 1984 1985 1986)
y=c(1979,1980,1981,1982,1983,1984,1985,1986)
i=c(3074,3135,3206,3267,3310,3362,3418,3500)
p=c(3292,3250,3230,3255,3266,3283,3300,3337)
err=i-p
plot(y,err)
err
xlab="Year"
plot(y,err)
xlab("Year")
plot(y,err,xlab="Year")
plot(y,err,xlab="Year",ylab="Forecast Error", main = "Error by Year")
sum(err)
sum(abs(err)
)
e2=err*err
sum(e2)
ape=100*abs(err)/i
sum(ape)/8
sum(ape)
year=c(1998, 1999, 2000, 2001, 2002)
actual=c(8,12,14,16,10)
a=c9,11.5,14,16.5,19)
a=c(9,11.5,14,16.5,19)
b=c(9.5,10.5,12,13,15)
erra=actual-a
errb=actual-b
aba=sum(abs(erra))
abb=sum(abs(errb))
aba/5
abb/5
erraerra
erra*erra
sum(erra*erra)/5
sum(errb*errb)/5
y=c(99,123,75,138,105,65,116)
ybar=mean(y)
yminus=y-ybar
yminus2=yminus*yminus
sum(yminus2)/6
s2=sum(yminus2)/6
s=sqrt(s2)
s2
s
25*25
bags=c(50.6,50.6,50.8,50.8,50.8,49.8,49.8,51.4,50.8,50.6,50.6,50.8,50.8,50.4,50.6,50.7,49.1,49,50.5,50.3,50.8,50.6,51.2,51.1,50.2,49.9,52.2,50.3,50.5,46.8,50.4,50.1,50.7,49.8,52.0,50.5)
bags=c(50.6,50.6,50.8,50.8,50.8,49.8,49.8,51.4,50.8,50.6,50.6,50.8,50.8,50.4,50.6,50.7,49.1,49,50.5,50.3,50.8,50.6,51.2,51.1,50.2,49.9,52.2,50.3,50.2,46.8,50.4,50.1,50.7,49.8,52.0,50.5)
bagbar=mean(bags)
bagminus=bags-bagbar
bagminus2=bagminus*bagminus
sum(bagminus2)/36
sum(bagminus2)/35
bags2=sum(bagminus2)/35
bagsval=sqrt(bags2)
hist(bags)
stem(bags)
bagbar
save.image("C:/Users/Justin/Desktop/Regression/HW1.RData")
rm(list=ls)
rm(list = ls())
D <- matrix(c(0, 9, 3, 6,11,
9,0, 7 , 5, 10,
3,7,0, 9,2,
6,5,9,0,8,
11,10,2,8,0), byrow=TRUE, ncol=5) #Complete linkage example
C <- hclust(as.dist(D), method = "complete") #Desired Result
plot(C)
X <- readMoments("T12-5.DAT",diag=TRUE)
C <- hclust(as.dist(X), method = "complete")
#Corresponds to the (12345) cluster, we are done.
#We see here that our results matched perfectly the complete linkage method in R
### 12.7
rm(list=ls())
X <- readMoments("T12-5.DAT",diag=TRUE)
install.packages("sem")
#Corresponds to the (12345) cluster, we are done.
#We see here that our results matched perfectly the complete linkage method in R
### 12.7
rm(list=ls())
library(sem)
X <- readMoments("T12-5.DAT",diag=TRUE)
C <- hclust(as.dist(X), method = "complete")
setwd("C:/Users/Justin/Desktop/Spring 2019/Multivariate-Stats/Project")
X <- readMoments("T12-5.DAT",diag=TRUE)
C <- hclust(as.dist(X), method = "complete")
plot(C)
rm(list = ls())
X <- matrix(c(0,1,0,0,0,
1,1,1,0,0,
0,0,0,1,1,
0,1,0,1,1,
1,0,1,1,1,
0,1,1,1,0), byrow=TRUE, ncol=5)
#Reagan Carter
sim1 <- matrix(c(0,0,
0,0), byrow=TRUE, ncol=2)
for(i in 1:5){
if(X[1,i]==1 && X[2,i]==1){
sim1[1,1]<- sim1[1,1]+1
}
if(X[1,i]==1 && X[2,i]==0){
sim1[1,2]<- sim1[1,2]+1
}
if(X[1,i]==0 && X[2,i]==1){
sim1[2,1]<- sim1[2,1]+1
}
if(X[1,i]==0 && X[2,i]==0){
sim1[2,2]<- sim1[2,2]+1
}
}
sim1
X[1,]
X[2,]
simCoef1 <- (sim1[1,1]+sim1[2,2])/sum(sim1)
rm(list = ls())
X <- matrix(c(0,1,0,0,0,
1,1,1,0,0,
0,0,0,1,1,
0,1,0,1,1,
1,0,1,1,1,
0,1,1,1,0), byrow=TRUE, ncol=5)
#Reagan Carter
a=1
b=2
sim1 <- matrix(c(0,0,
0,0), byrow=TRUE, ncol=2)
for(i in 1:5){
if(X[a,i]==1 && X[b,i]==1){
sim1[1,1]<- sim1[1,1]+1
}
if(X[a,i]==1 && X[b,i]==0){
sim1[1,2]<- sim1[1,2]+1
}
if(X[a,i]==0 && X[b,i]==1){
sim1[2,1]<- sim1[2,1]+1
}
if(X[a,i]==0 && X[b,i]==0){
sim1[2,2]<- sim1[2,2]+1
}
}
for(i in 1:5){
if(X[a,i]==1 && X[b,i]==1){
sim1[1,1]<- sim1[1,1]+1
}
if(X[a,i]==1 && X[b,i]==0){
sim1[1,2]<- sim1[1,2]+1
}
if(X[a,i]==0 && X[b,i]==1){
sim1[2,1]<- sim1[2,1]+1
}
if(X[a,i]==0 && X[b,i]==0){
sim1[2,2]<- sim1[2,2]+1
}
}
simCoef1 <- (sim1[1,1]+sim1[2,2])/sum(sim1)
a=1
b=2
sim1 <- matrix(c(0,0,
0,0), byrow=TRUE, ncol=2)
for(i in 1:5){
if(X[a,i]==1 && X[b,i]==1){
sim1[1,1]<- sim1[1,1]+1
}
if(X[a,i]==1 && X[b,i]==0){
sim1[1,2]<- sim1[1,2]+1
}
if(X[a,i]==0 && X[b,i]==1){
sim1[2,1]<- sim1[2,1]+1
}
if(X[a,i]==0 && X[b,i]==0){
sim1[2,2]<- sim1[2,2]+1
}
}
(simCoef1 <- (sim1[1,1]+sim1[2,2])/sum(sim1))
a=1
b=3
sim1 <- matrix(c(0,0,
0,0), byrow=TRUE, ncol=2)
for(i in 1:5){
if(X[a,i]==1 && X[b,i]==1){
sim1[1,1]<- sim1[1,1]+1
}
if(X[a,i]==1 && X[b,i]==0){
sim1[1,2]<- sim1[1,2]+1
}
if(X[a,i]==0 && X[b,i]==1){
sim1[2,1]<- sim1[2,1]+1
}
if(X[a,i]==0 && X[b,i]==0){
sim1[2,2]<- sim1[2,2]+1
}
}
(simCoef1 <- (sim1[1,1]+sim1[2,2])/sum(sim1))
## 12.7 Stock Data
rm(list=ls())
X <- matrix(c(1,.63,.51,.12,.16,
.63,1,.57,.32,.21,
.51,.57,1,.18,.15,
.12,.32,.18,1,.68,
.16,.21,.15,.68,1), byrow = TRUE, ncol=5)
C <- hclust(as.dist(X), method = "complete")
plot(C)
row.names(X) <- c("JP Morgan", "Citibank","Wells Fargo", "Royal DutchShell","ExxonMobil")
C <- hclust(as.dist(X), method = "complete")
plot(C)
#We see based on this analysis that JP Morgan and Royal DutchShell are closely related, followed by Wells fargo and ExxonMobil
C <- hclust(as.dist(X), method = "average")
plot(C)
## 12.15
#This one will be k-means clustering
rm(list=ls())
X <- read.table("T11-9.dat", headers=FALSE)
X <- read.table("T11-9.dat", headers=FALSE)
X <- read.table("T11-9.dat")
X
Y<-X[,3:]
Y<-X[,3:11]
k1 <- kmeans(Y,2)
Y
k1 <- kmeans(X,2)
is.n(y)
is.na(Y)
is.finite(Y)
is.finite(Y[,1])
is.finite(Y[,2])
for(i in 1:11){}
for(i in 1:11){
is.finite(Y[,i])
}
for(i in 1:9){
is.finite(Y[,i])
}
for(i in 1:9){
print(is.finite(Y[,i]))
}
## 12.15
#This one will be k-means clustering
rm(list=ls())
X <- read.table("T11-9.dat")
Y<-X[,3:11]
k1 <- kmeans(X,2)
## 12.15
#This one will be k-means clustering
rm(list=ls())
X <- read.table("T11-9.dat")
X<-X[-c(1,2)]
k1 <- kmeans(X,2)
rm(list=ls())
X <- read.table("T11-9.dat")
X<-X[-c(1,2,11)]
k1 <- kmeans(X,2)
X<-as.matrix(X)
k1 <- kmeans(X,2)
X
X <- strtoi(X)
X
## 12.15
#This one will be k-means clustering
rm(list=ls())
X <- read.table("T11-9.dat")
X<-X[-c(1,2)]
X
X<-as.matrix(X)
k1 <- kmeans(X,2)
X
rm(list=ls())
X <- read.table("T11-9.dat")
X<-X[-c(1,2)]
X<-as.matrix(X)
X
rm(list=ls())
X <- read.table("T11-9.dat")
X<-X[-c(1,2)]
k1 <- kmeans(X,2)
k1 <- kmeans(X[,1],2)
print(k1)
k1 <- kmeans(X[,1:9],2)
k1 <- kmeans(X[,1:8],2)
k1 <- kmeans(X[,1:7],2)
k1 <- kmeans(X[,1:6],2)
k1 <- kmeans(X[,1:5],2)
print(k1)
k1$cluster
k2 <- kmeans(X[,1:5],3)
print(k2)
k3 <- kmeans(X[,1:5],4)
print(k3)
hist(k1)
hist(k1$cluster)
hist(k2$cluster)
hist(k3$cluster)
# 12.26 Maloi Farm
rm(list=ls())
X <- read.table("T8-7.DAT", header=FALSE)
X
X <- X[-c(25,34,69,72),]
avg <- hclust(X, method="average")
plot(avg)
X
# 12.26 Maloi Farm
rm(list=ls())
X <- read.table("T8-7.DAT", header=FALSE)
X <- X[-c(25,34,69,72),-c(7:9)]
avg <- hclust(X, method="average")
plot(avg)
76*6
rm(list=ls())
X <- read.table("T8-7.DAT", header=FALSE)
X <- X[-c(25,34,69,72),-c(1,7:9)]
avg <- hclust(X, method="average")
plot(avg)
rm
# 12.26 Maloi Farm
rm(list=ls())
X <- read.table("T8-7.DAT", header=FALSE)
X <- X[-c(25,34,69,72)]#,-c(1,7:9)]
X <- as.matrix(X)
X
avg <- hclust(X, method="average")
# 12.26 Maloi Farm
rm(list=ls())
X <- read.csv("T8-7.DAT", header=FALSE)
X <- X[-c(25,34,69,72)]#,-c(1,7:9)]
X <- as.matrix(X)
avg <- hclust(X, method="average")
D <- dist(X,"euclidean")
D <- dist(X,method="euclidean")
# 12.26 Maloi Farm
rm(list=ls())
X <- read.csv("T8-7.DAT", header=FALSE)
X <- X[-c(25,34,69,72)]#,-c(1,7:9)]
D <- dist(X,method="euclidean")
avg <- hclust(D, method="average")
D
#D <- dist(X,method="euclidean")
avg <- hclust(as.dist(X), method="average")
D <- matrix(0, nrow= 72, ncol=72)
D
for(i in 1:72){
for(j in 1:72){
d <- sqrt(sum((X[i,]-X[j,])^2))
D[i,j] <- d
}
}
D
X[1,]
# 12.26 Maloi Farm
rm(list=ls())
X <- read.csv("T8-7.DAT", header=FALSE)
X
# 12.26 Maloi Farm
rm(list=ls())
X <- read.table("T8-7.DAT", header=FALSE)
X <- X[-c(25,34,69,72)]#,-c(1,7:9)]
D <- matrix(0, nrow= 72, ncol=72)
X <- read.table("T8-7.DAT", header=FALSE)
X <- X[-c(25,34,69,72),]#,-c(1,7:9)]
D <- matrix(0, nrow= 72, ncol=72)
for(i in 1:72){
for(j in 1:72){
d <- sqrt(sum((X[i,]-X[j,])^2))
D[i,j] <- d
}
}
D <- matrix(0, nrow= 72, ncol=72)
for(i in 1:72){
for(j in 1:72){
d <- sqrt(sum((X[i,]-X[j,])^2))
D[i,j] <- d
}
}
D <- matrix(0, nrow= 72, ncol=72)
for(i in 1:72){
for(j in i:72){
d <- sqrt(sum((X[i,]-X[j,])^2))
D[i,j] <- d
}
}
sum(D)
D
D<-t(D)
D <- as.dist(D)
avg <- hclust(D, method="average")
plot(avg)
ward <- hclust(D, method = "ward.D")
plot(ward)
plot(avg)
k1 <- kmeans(X,5)
print(k1)
plot(k1)
k1$cl
silhouette(k1$cl)
install.packages("HSAUR")
## Based on the Ward method, we see tat there are around 7 base clusters of farms based on Euclidean Distance
library(HSAUR)
silhouette(k1$cl)
install.packages("cluster")
silhouette(k1$cl)
## Based on the Ward method, we see tat there are around 7 base clusters of farms based on Euclidean Distance
library(cluster)
k1 <- kmeans(X,5)
silhouette(k1$cl)
silhouette(k1$cl,D)
s=silhouette(k1$cl,D)
plot(s)
plot(X,k1$cluster)
plot(X[,1],X[,2],k1$cluster)
clusplot(X,k1$cluster)
k2 <- kmeans(X,6)
s2=silhouette(k2$cl,D)
plot(s2)
clusplot(X,k2$cluster)
